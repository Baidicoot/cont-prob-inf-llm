{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "import math\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from algos.annealed_smc import AnnealedSMC\n",
    "from utils import build_relaxed_single_token_prior, build_suffix_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_relaxed_prior_factory(\n",
    "    model: GPT2LMHeadModel,\n",
    "    tokenizer: GPT2Tokenizer,\n",
    "    device: torch.device,\n",
    "):\n",
    "    \"\"\"Return a callable that, for any σ, yields functional prior handles.\"\"\"\n",
    "\n",
    "    # Static objects shared across all σ\n",
    "    with torch.no_grad():\n",
    "        bos = torch.tensor([[tokenizer.eos_token_id]], device=device)\n",
    "        logits = model(bos).logits[:, -1]                # (1,V)\n",
    "        prior_probs = logits.softmax(-1).squeeze(0)      # (V,)\n",
    "    E: torch.Tensor = model.transformer.wte.weight.detach()  # (V,d)\n",
    "    V, d = E.shape\n",
    "\n",
    "    # def factory():\n",
    "    def logp(z: torch.Tensor, σ: float) -> torch.Tensor:\n",
    "        const = -0.5 * d * math.log(2 * math.pi * σ * σ)\n",
    "        inv_var = 1.0 / (σ * σ)\n",
    "        diff = z.unsqueeze(1) - E.unsqueeze(0)        # (N,V,d)\n",
    "        mahal = diff.square().sum(-1)                # (N,V)\n",
    "        log_gauss = const - 0.5 * inv_var * mahal    # (N,V)\n",
    "        log_weighted = log_gauss + prior_probs.log().unsqueeze(0)\n",
    "        return torch.logsumexp(log_weighted, dim=-1) # (N,)\n",
    "\n",
    "    def grad(z: torch.Tensor, σ: float) -> torch.Tensor:\n",
    "        return torch.func.grad(lambda x: logp(x, σ).sum())(z)\n",
    "\n",
    "    def sample(num: int, σ: float) -> torch.Tensor:\n",
    "        cat = Categorical(prior_probs)\n",
    "        tokens = cat.sample((num,))                  # (num,)\n",
    "        base = E[tokens]                             # (num,d)\n",
    "        noise = torch.randn_like(base) * σ\n",
    "        return base + noise\n",
    "\n",
    "    return logp, grad, sample\n",
    "    # return factory\n",
    "\n",
    "def build_suffix_likelihood(\n",
    "    model: GPT2LMHeadModel,\n",
    "    tokenizer: GPT2Tokenizer,\n",
    "    # suffix: str,\n",
    "    suffix_ids: List[int],\n",
    "    device: torch.device,\n",
    "):\n",
    "    \"\"\"Return (log‑likelihood, grad‑likelihood) functions depending only on *z*.\"\"\"\n",
    "\n",
    "    # suffix_ids = tokenizer.encode(suffix, add_special_tokens=False)\n",
    "    ids_tensor = torch.tensor(suffix_ids, device=device)\n",
    "    bos_tensor = torch.tensor([tokenizer.bos_token_id], device=device)\n",
    "    with torch.no_grad():\n",
    "        suffix_embeds = model.transformer.wte(ids_tensor)  # (L,d)\n",
    "        bos_embeds = model.transformer.wte(bos_tensor)    # (1,d)\n",
    "    L = len(suffix_ids)\n",
    "\n",
    "    def logp(z: torch.Tensor) -> torch.Tensor:\n",
    "        N, d = z.shape\n",
    "        z_seq = z.unsqueeze(1)                            # (N,1,d)\n",
    "        suffix_expand = suffix_embeds.unsqueeze(0).expand(N, -1, -1)  # (N,L,d)\n",
    "        bos_expand = bos_embeds.unsqueeze(0).expand(N, -1, -1)        # (N,1,d)\n",
    "        inputs = torch.cat([bos_expand, z_seq, suffix_expand], dim=1)  # (N,2+L,d)\n",
    "        logits = model(inputs_embeds=inputs).logits                   # (N,2+L,V)\n",
    "\n",
    "        log_p = torch.zeros(N, device=device)\n",
    "        for pos, tok_id in enumerate(suffix_ids):\n",
    "            step_logits = logits[:, pos+1, :]\n",
    "            log_p += torch.log_softmax(step_logits, dim=-1)[:, tok_id]\n",
    "        return log_p                                      # (N,)\n",
    "\n",
    "    def grad(z: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.func.grad(lambda x: logp(x).sum())(z)\n",
    "\n",
    "    return logp, grad, suffix_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2').to(device, dtype=torch.float32)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# suffix = \" went to the shop\"\n",
    "# suffix_ids = tokenizer.encode(suffix, add_special_tokens=False)\n",
    "\n",
    "# log_prior, grad_log_prior, sample_prior = build_relaxed_single_token_prior(model, tokenizer, device)\n",
    "# log_like, grad_log_like = build_suffix_likelihood(model, tokenizer, suffix_ids, device)\n",
    "\n",
    "# def log_target(x, sigma):\n",
    "#     # with torch.no_grad():\n",
    "#     #     return log_like(x) + log_prior(x, sigma)\n",
    "#     with torch.no_grad():\n",
    "#         return log_prior(x, sigma)\n",
    "\n",
    "# def grad_log_target(x, sigma):\n",
    "#     # with torch.no_grad():\n",
    "#     #     return grad_log_like(x) + grad_log_prior(x, sigma)\n",
    "#     with torch.no_grad():\n",
    "#         return grad_log_prior(x, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = \" went to the shop\"\n",
    "suffix_ids = tokenizer.encode(suffix, add_special_tokens=False)\n",
    "\n",
    "log_prior, grad_log_prior, sample_prior = build_relaxed_prior_factory(model, tokenizer, device)\n",
    "log_like, grad_log_like, suffix_ids = build_suffix_likelihood(model, tokenizer, suffix_ids, device)\n",
    "\n",
    "def log_target(x, sigma):\n",
    "    with torch.no_grad():\n",
    "        return log_like(x) + log_prior(x, sigma)\n",
    "        # return log_prior(x, sigma)\n",
    "\n",
    "def grad_log_target(x, sigma):\n",
    "    with torch.no_grad():\n",
    "        return grad_log_like(x) + grad_log_prior(x, sigma)\n",
    "        # return grad_log_prior(x, sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 32\n",
    "d = 768 # gpt2 embedding dimension\n",
    "sigma0 = 10.0\n",
    "sigma_target = 0.1\n",
    "\n",
    "def sample_initial_particles(N, d):\n",
    "    return sample_prior(N, sigma0)\n",
    "\n",
    "sampler = AnnealedSMC(\n",
    "    N=N,\n",
    "    x_dim=d,\n",
    "    sigma_0=sigma0,\n",
    "    sigma_target=sigma_target,\n",
    "    alpha=0.5,\n",
    "    mala_step_size=0.2,\n",
    "    mala_steps=3,\n",
    "    ess_min_frac=0.5,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "final_particles = sampler.run(init_sampler=sample_initial_particles, log_target=log_target, grad_log_target=grad_log_target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
